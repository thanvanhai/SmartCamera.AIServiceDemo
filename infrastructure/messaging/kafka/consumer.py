# /home/haicoi/SmartCamera.AIServiceDemo/infrastructure/messaging/kafka/consumer.py

import asyncio
import json
import logging
import time
from typing import Dict, Any, Optional, List, AsyncGenerator
from dataclasses import dataclass

# Sá»­ dá»¥ng try-except Ä‘á»ƒ khÃ´ng bá»‹ lá»—i náº¿u chÆ°a cÃ i aiokafka
try:
    from aiokafka import AIOKafkaConsumer, ConsumerRecord
    from aiokafka.errors import KafkaError, CommitFailedError, IllegalStateError
    from aiokafka.structs import TopicPartition
except ImportError:
    AIOKafkaConsumer = None
    # ... (giá»¯ nguyÃªn pháº§n mock cá»§a báº¡n)

# --- CÃ¡c lá»›p dá»¯ liá»‡u (Data Classes) ---
# Giá»¯ nguyÃªn cÃ¡c dataclass vÃ¬ chÃºng giÃºp cáº¥u trÃºc code rÃµ rÃ ng
@dataclass
class MessageMetadata:
    """ÄÃ³ng gÃ³i metadata cá»§a message"""
    topic: str
    partition: int
    offset: int
    timestamp: float
    key: Optional[str]

@dataclass
class ConsumedMessage:
    """ÄÃ³ng gÃ³i message Ä‘Ã£ Ä‘Æ°á»£c consume"""
    value: Dict[str, Any]
    metadata: MessageMetadata
    raw_record: Any  # Giá»¯ láº¡i record gá»‘c náº¿u cáº§n


class KafkaConsumer:
    """
    Lá»›p Kafka Consumer báº¥t Ä‘á»“ng bá»™, Ä‘Æ°á»£c tÃ¡i cáº¥u trÃºc Ä‘á»ƒ cÃ³ cÃ¡ch khá»Ÿi táº¡o tÆ°á»ng minh.
    """
    
    def __init__(
        self,
        bootstrap_servers: str,
        group_id: str,
        # CÃ¡c tham sá»‘ cáº¥u hÃ¬nh khÃ¡c vá»›i giÃ¡ trá»‹ máº·c Ä‘á»‹nh há»£p lÃ½
        auto_offset_reset: str = 'latest',
        enable_auto_commit: bool = True,
        auto_commit_interval_ms: int = 1000,
        session_timeout_ms: int = 30000,
        heartbeat_interval_ms: int = 3000,
        fetch_max_bytes: int = 52428800  # 50MB
    ):
        """
        Khá»Ÿi táº¡o Consumer vá»›i cÃ¡c tham sá»‘ rÃµ rÃ ng thay vÃ¬ má»™t config dictionary.
        """
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # --- THAY Äá»”I CHÃNH Náº°M á»ž ÄÃ‚Y ---
        # GÃ¡n trá»±c tiáº¿p cÃ¡c tham sá»‘ thay vÃ¬ dÃ¹ng .get() tá»« dictionary
        self.bootstrap_servers = bootstrap_servers
        self.group_id = group_id
        self.auto_offset_reset = auto_offset_reset
        self.enable_auto_commit = enable_auto_commit
        self.auto_commit_interval_ms = auto_commit_interval_ms
        self.session_timeout_ms = session_timeout_ms
        self.heartbeat_interval_ms = heartbeat_interval_ms
        self.fetch_max_bytes = fetch_max_bytes
        
        self.consumer: Optional[AIOKafkaConsumer] = None
        self.is_connected = False
        
        # CÃ¡c thÃ´ng sá»‘ giÃ¡m sÃ¡t (monitoring)
        self.messages_consumed = 0
        self.start_time = None
        self.subscribed_topics = set()
        self.is_consuming = False

    async def connect(self):
        """Káº¿t ná»‘i tá»›i Kafka cluster"""
        if self.is_connected:
            self.logger.warning("Consumer Ä‘Ã£ Ä‘Æ°á»£c káº¿t ná»‘i.")
            return
            
        self.logger.info(f"ðŸ”— Äang káº¿t ná»‘i Kafka Consumer tá»›i {self.bootstrap_servers}...")
        try:
            self.consumer = AIOKafkaConsumer(
                # Topics sáº½ Ä‘Æ°á»£c truyá»n vÃ o khi gá»i subscribe() hoáº·c consume()
                bootstrap_servers=self.bootstrap_servers,
                group_id=self.group_id,
                value_deserializer=self._deserialize_message,
                key_deserializer=lambda k: k.decode('utf-8') if k else None,
                auto_offset_reset=self.auto_offset_reset,
                enable_auto_commit=self.enable_auto_commit,
                auto_commit_interval_ms=self.auto_commit_interval_ms,
                session_timeout_ms=self.session_timeout_ms,
                heartbeat_interval_ms=self.heartbeat_interval_ms,
                fetch_max_bytes=self.fetch_max_bytes
            )
            await self.consumer.start()
            self.is_connected = True
            self.start_time = time.time()
            self.logger.info(f"âœ… Káº¿t ná»‘i thÃ nh cÃ´ng (group: {self.group_id})")
        except Exception as e:
            self.logger.error(f"âŒ Káº¿t ná»‘i tháº¥t báº¡i: {e}", exc_info=True)
            self.is_connected = False
            raise

    async def subscribe(self, topics: List[str]):
        """ÄÄƒng kÃ½ (subscribe) vÃ o cÃ¡c topic"""
        if not self.is_connected:
            raise RuntimeError("Consumer chÆ°a káº¿t ná»‘i. HÃ£y gá»i connect() trÆ°á»›c.")
        
        self.logger.info(f"ðŸ“‹ ÄÄƒng kÃ½ vÃ o cÃ¡c topic: {topics}")
        self.consumer.subscribe(topics)
        self.subscribed_topics.update(topics)

    async def consume(self, topic: str) -> AsyncGenerator[ConsumedMessage, None]:
        """
        Nháº­n message liÃªn tá»¥c tá»« má»™t topic cá»¥ thá»ƒ dÆ°á»›i dáº¡ng async generator.
        """
        if not self.is_connected:
            raise RuntimeError("Consumer chÆ°a káº¿t ná»‘i. HÃ£y gá»i connect() trÆ°á»›c.")
        
        if topic not in self.subscribed_topics:
            await self.subscribe([topic])
        
        self.is_consuming = True
        self.logger.info(f"ðŸ“¥ Báº¯t Ä‘áº§u nháº­n message tá»« topic: {topic}")
        try:
            async for record in self.consumer:
                if not self.is_consuming: break
                if record.topic != topic: continue
                
                self.messages_consumed += 1
                yield ConsumedMessage(
                    value=record.value,
                    metadata=MessageMetadata(
                        topic=record.topic,
                        partition=record.partition,
                        offset=record.offset,
                        timestamp=record.timestamp / 1000.0,
                        key=record.key
                    ),
                    raw_record=record
                )
        except Exception as e:
            self.logger.error(f"âŒ Lá»—i nghiÃªm trá»ng khi consume tá»« {topic}: {e}", exc_info=True)
            raise
        finally:
            self.is_consuming = False
            self.logger.info(f"ðŸ›‘ ÄÃ£ dá»«ng nháº­n message tá»« topic: {topic}")

    async def consume_batch(self, topic: str, max_messages: int = 100, timeout_ms: int = 5000) -> List[ConsumedMessage]:
        """Nháº­n má»™t lÃ´ (batch) message tá»« topic."""
        # Logic cá»§a hÃ m nÃ y khÃ´ng thay Ä‘á»•i, nÃ³ váº«n hoáº¡t Ä‘á»™ng tá»‘t
        batch = []
        try:
            async for message in self.consume(topic):
                batch.append(message)
                if len(batch) >= max_messages:
                    break
            # LÆ°u Ã½: aiokafka tá»± quáº£n lÃ½ timeout, nÃªn khÃ´ng cáº§n check time thá»§ cÃ´ng phá»©c táº¡p
        except Exception as e:
            self.logger.error(f"âŒ Lá»—i khi consume batch tá»« {topic}: {e}")
        return batch

    async def close(self):
        """Ngáº¯t káº¿t ná»‘i an toÃ n"""
        if not self.is_connected: return
        
        self.logger.info("ðŸ”Œ Äang ngáº¯t káº¿t ná»‘i Kafka Consumer...")
        self.is_consuming = False
        if self.consumer:
            await self.consumer.stop()
        self.is_connected = False
        self.logger.info("âœ… ÄÃ£ ngáº¯t káº¿t ná»‘i.")
    
    @staticmethod
    def _deserialize_message(raw_data: Optional[bytes]) -> Optional[Dict[str, Any]]:
        """Giáº£i mÃ£ message tá»« JSON bytes"""
        if not raw_data: return None
        try:
            return json.loads(raw_data.decode('utf-8'))
        except (json.JSONDecodeError, UnicodeDecodeError) as e:
            logging.getLogger('KafkaConsumer').error(f"Lá»—i giáº£i mÃ£ message: {e}")
            return {"error": "deserialization_failed", "raw_data": raw_data.decode('utf-8', errors='ignore')}

    def get_metrics(self) -> Dict[str, Any]:
        """Láº¥y cÃ¡c chá»‰ sá»‘ hoáº¡t Ä‘á»™ng cá»§a consumer"""
        uptime = time.time() - self.start_time if self.start_time else 0
        return {
            'is_connected': self.is_connected,
            'is_consuming': self.is_consuming,
            'group_id': self.group_id,
            'subscribed_topics': list(self.subscribed_topics),
            'messages_consumed': self.messages_consumed,
            'uptime_seconds': uptime,
            'messages_per_second': self.messages_consumed / uptime if uptime > 0 else 0,
        }

    def __repr__(self):
        return f"KafkaConsumer(connected={self.is_connected}, group={self.group_id}, topics={list(self.subscribed_topics)})"