import asyncio
import json
import logging
import time
import os
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from aiokafka import AIOKafkaProducer
from aiokafka.admin import AIOKafkaAdminClient, NewTopic
from aiokafka.errors import TopicAlreadyExistsError
import hashlib
import cv2
import numpy as np
from typing import Optional
from concurrent.futures import ThreadPoolExecutor

# --- C·∫•u h√¨nh logging c∆° b·∫£n ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class KafkaPublisher:
    """
    M·ªôt class publisher hi·ªáu su·∫•t cao cho Kafka, ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ x·ª≠ l√Ω frame t·ª´ camera.
    - L∆∞u frame ·∫£nh ra ƒëƒ©a local (Claim-Check pattern).
    - G·ª≠i metadata c·ªßa frame (ƒë∆∞·ªùng d·∫´n, k√≠ch th∆∞·ªõc) t·ªõi Kafka.
    - T·ª± ƒë·ªông d·ªçn d·∫πp file c≈© d·ª±a tr√™n tu·ªïi v√† t·ªïng dung l∆∞·ª£ng l∆∞u tr·ªØ.
    - ƒê∆∞·ª£c t·ªëi ∆∞u cho m√¥i tr∆∞·ªùng production v·ªõi c√°c t√≠nh nƒÉng: idempotence, acks='all',
      gi·ªõi h·∫°n I/O ƒë·ªìng th·ªùi v√† x·ª≠ l√Ω l·ªói retry.
    """
    def __init__(
        self,
        bootstrap_servers: str,
        topic_name: str = "smartcamera",
        num_partitions: int = 10,
        replication_factor: int = 1,
        frame_storage_dir: str = "./frames",
        max_storage_days: int = 7,
        max_storage_gb: float = 20.0,
        cleanup_interval: int = 3600, # 1 gi·ªù
        use_date_folders: bool = True,
        max_retry: int = 3,
        max_frame_concurrent: int = 16, # Gi·ªõi h·∫°n s·ªë frame ghi xu·ªëng ƒëƒ©a ƒë·ªìng th·ªùi
    ):
        self.bootstrap_servers = bootstrap_servers
        self.topic_name = topic_name
        self.num_partitions = num_partitions
        self.replication_factor = replication_factor
        self.frame_storage_dir = Path(frame_storage_dir)
        self.max_storage_days = max_storage_days
        self.max_storage_gb = max_storage_gb
        self.cleanup_interval = cleanup_interval
        self.use_date_folders = use_date_folders
        self.max_retry = max_retry

        # ƒê·∫£m b·∫£o th∆∞ m·ª•c l∆∞u tr·ªØ t·ªìn t·∫°i
        self.frame_storage_dir.mkdir(parents=True, exist_ok=True)

        self.producer: Optional[AIOKafkaProducer] = None
        self.admin_client: Optional[AIOKafkaAdminClient] = None
        self.topic_created = False

        # Thread pool ƒë·ªÉ ch·∫°y c√°c t√°c v·ª• blocking I/O (ghi file) m√† kh√¥ng l√†m ngh·∫Ωn event loop
        self.executor = ThreadPoolExecutor(max_workers=min(32, (os.cpu_count() or 4) * 2))
        logger.info(f"ThreadPoolExecutor ƒë∆∞·ª£c kh·ªüi t·∫°o v·ªõi {self.executor._max_workers} workers.")

        # Semaphore gi·ªõi h·∫°n s·ªë l∆∞·ª£ng t√°c v·ª• ghi file ƒë·ªìng th·ªùi ƒë·ªÉ tr√°nh qu√° t·∫£i ƒëƒ©a
        self.frame_semaphore = asyncio.Semaphore(max_frame_concurrent)

        self.running = False
        self.cleanup_task: Optional[asyncio.Task] = None

        # S·ª≠ d·ª•ng asyncio.Lock ƒë·ªÉ ƒë·∫£m b·∫£o an to√†n lu·ªìng khi c·∫≠p nh·∫≠t stats
        self.stats_lock = asyncio.Lock()
        self.stats = defaultdict(lambda: {"frames": 0, "events": 0})

        # Cache ƒë·ªÉ l∆∞u k·∫øt qu·∫£ ph√¢n v√πng cho t·ª´ng camera_id, tr√°nh t√≠nh to√°n l·∫°i hash
        self.partition_cache = {}

    # ------------------- C√°c h√†m ti·ªán √≠ch (Utilities) -------------------

    def _get_storage_path(self, camera_id: str, timestamp: int) -> Path:
        """T·∫°o v√† tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n l∆∞u tr·ªØ frame d·ª±a tr√™n c·∫•u h√¨nh."""
        if self.use_date_folders:
            # S·∫Øp x·∫øp frame v√†o c√°c th∆∞ m·ª•c YYYY/MM/DD ƒë·ªÉ d·ªÖ qu·∫£n l√Ω
            date_str = datetime.fromtimestamp(timestamp / 1000).strftime("%Y/%m/%d")
            folder = self.frame_storage_dir / date_str / camera_id
        else:
            folder = self.frame_storage_dir / camera_id
        folder.mkdir(parents=True, exist_ok=True)
        return folder

    def _save_frame_sync(self, frame: np.ndarray, path: Path) -> Optional[int]:
        """
        H√†m ƒë·ªìng b·ªô (blocking) ƒë·ªÉ ghi frame ra file.
        Tr·∫£ v·ªÅ k√≠ch th∆∞·ªõc file n·∫øu th√†nh c√¥ng, None n·∫øu th·∫•t b·∫°i.
        """
        try:
            # Ghi ·∫£nh JPEG v·ªõi ch·∫•t l∆∞·ª£ng 70 v√† t·ªëi ∆∞u h√≥a ƒë·ªÉ gi·∫£m dung l∆∞·ª£ng
            cv2.imwrite(str(path), frame, [cv2.IMWRITE_JPEG_QUALITY, 70, cv2.IMWRITE_JPEG_OPTIMIZE, 1])
            # Tr·∫£ v·ªÅ k√≠ch th∆∞·ªõc file ngay sau khi ghi th√†nh c√¥ng
            return path.stat().st_size
        except Exception as e:
            logger.error(f"Th·∫•t b·∫°i khi l∆∞u frame {path}: {e}")
            return None

    async def _save_frame_async(self, frame: np.ndarray, path: Path) -> Optional[int]:
        """
        H√†m b·∫•t ƒë·ªìng b·ªô (non-blocking) ƒë·ªÉ ghi frame.
        S·ª≠ d·ª•ng semaphore ƒë·ªÉ gi·ªõi h·∫°n s·ªë l∆∞·ª£ng ghi ƒë·ªìng th·ªùi v√† executor ƒë·ªÉ ch·∫°y h√†m sync.
        """
        async with self.frame_semaphore:
            loop = asyncio.get_event_loop()
            # Ch·∫°y h√†m blocking _save_frame_sync trong m·ªôt thread ri√™ng
            file_size = await loop.run_in_executor(self.executor, self._save_frame_sync, frame, path)
            return file_size

    def _camera_partitioner(self, key_bytes, all_partitions, available_partitions):
        """
        Partitioner t√πy ch·ªânh: ƒë·∫£m b·∫£o t·∫•t c·∫£ message t·ª´ c√πng m·ªôt camera_id
        s·∫Ω ƒëi v√†o c√πng m·ªôt partition. ƒêi·ªÅu n√†y gi·ªØ ƒë√∫ng th·ª© t·ª± message cho m·ªói camera.
        """
        if key_bytes is None:
            return available_partitions[0] if available_partitions else all_partitions[0]

        key_str = key_bytes.decode('utf-8')
        # Ki·ªÉm tra cache tr∆∞·ªõc khi t√≠nh to√°n hash
        if key_str not in self.partition_cache:
            h = int(hashlib.md5(key_bytes).hexdigest(), 16)
            partition = all_partitions[h % len(all_partitions)]
            self.partition_cache[key_str] = partition

        return self.partition_cache[key_str]

    # ------------------- Qu·∫£n l√Ω k·∫øt n·ªëi Kafka -------------------

    async def connect(self):
        """Kh·ªüi t·∫°o k·∫øt n·ªëi t·ªõi Kafka, t·∫°o topic v√† b·∫Øt ƒë·∫ßu c√°c t√°c v·ª• n·ªÅn."""
        self.producer = AIOKafkaProducer(
            bootstrap_servers=self.bootstrap_servers,
            value_serializer=lambda v: json.dumps(v, separators=(",", ":")).encode("utf-8"),
            compression_type="gzip", # N√©n message ƒë·ªÉ ti·∫øt ki·ªám bƒÉng th√¥ng v√† dung l∆∞·ª£ng
            linger_ms=50,            # Ch·ªù 50ms ƒë·ªÉ gom message th√†nh batch
            max_batch_size=32768,    # ƒê√∫ng tham s·ªë
            partitioner=self._camera_partitioner,
            # C√†i ƒë·∫∑t quan tr·ªçng cho production:
            acks='all',              # ƒê·∫£m b·∫£o message ƒë√£ ƒë∆∞·ª£c replicate ƒë·∫ßy ƒë·ªß
            enable_idempotence=True, # ƒê·∫£m b·∫£o message ƒë∆∞·ª£c g·ª≠i ƒë√∫ng m·ªôt l·∫ßn (exactly-once)
        )
        await self.producer.start()

        self.admin_client = AIOKafkaAdminClient(bootstrap_servers=self.bootstrap_servers)
        await self.admin_client.start()

        await self.ensure_topic_exists()
        self.running = True
        self.cleanup_task = asyncio.create_task(self._periodic_cleanup())
        # Ch·∫°y d·ªçn d·∫πp m·ªôt l·∫ßn ngay khi kh·ªüi ƒë·ªông
        await self._cleanup_old_files()

    async def ensure_topic_exists(self):
        """ƒê·∫£m b·∫£o topic Kafka t·ªìn t·∫°i v·ªõi ƒë√∫ng c·∫•u h√¨nh."""
        if self.topic_created:
            return
        try:
            topic = NewTopic(
                name=self.topic_name,
                num_partitions=self.num_partitions,
                replication_factor=self.replication_factor,
                topic_configs={
                    "cleanup.policy": "delete",
                    "retention.ms": str(self.max_storage_days * 24 * 3600 * 1000),
                    "compression.type": "gzip",
                }
            )
            await self.admin_client.create_topics([topic])
            logger.info(f"‚úÖ ƒê√£ t·∫°o Kafka topic '{self.topic_name}'")
        except TopicAlreadyExistsError:
            logger.info(f"‚ÑπÔ∏è Topic '{self.topic_name}' ƒë√£ t·ªìn t·∫°i.")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫°o topic: {e}")
        self.topic_created = True

    async def _send_with_retry(self, message: dict, key: str) -> bool:
        """G·ª≠i message t·ªõi Kafka v·ªõi c∆° ch·∫ø th·ª≠ l·∫°i (retry)."""
        for attempt in range(self.max_retry):
            try:
                # send_and_wait s·∫Ω ch·ªù x√°c nh·∫≠n t·ª´ broker
                await self.producer.send_and_wait(self.topic_name, value=message, key=key.encode('utf-8'))
                return True
            except Exception as e:
                logger.warning(f"G·ª≠i message l·∫ßn {attempt + 1} th·∫•t b·∫°i: {e}")
                await asyncio.sleep(0.5 * (attempt + 1)) # Ch·ªù l√¢u h∆°n sau m·ªói l·∫ßn th·∫•t b·∫°i
        logger.error(f"G·ª≠i message cho key '{key}' th·∫•t b·∫°i sau {self.max_retry} l·∫ßn th·ª≠.")
        return False

    # ------------------- C√°c h√†m xu·∫•t b·∫£n (Publishing) -------------------

    async def publish_frame(self, camera_id: str, frame: np.ndarray, metadata: Optional[dict] = None) -> bool:
        """
        L∆∞u frame ra ƒëƒ©a v√† publish metadata c·ªßa n√≥ l√™n Kafka.
        ƒê√¢y l√† vi·ªác tri·ªÉn khai Claim-Check Pattern.
        """
        timestamp = int(time.time() * 1000)
        folder = self._get_storage_path(camera_id, timestamp)
        filename = f"{timestamp}.jpg"
        path = folder / filename

        # B∆∞·ªõc 1: L∆∞u file (payload l·ªõn) ra h·ªá th·ªëng l∆∞u tr·ªØ ngo√†i
        file_size = await self._save_frame_async(frame, path)
        if file_size is None:
            logger.error(f"Kh√¥ng th·ªÉ l∆∞u frame cho camera {camera_id}")
            return False

        # B∆∞·ªõc 2: T·∫°o message (claim check) ch·ª©a th√¥ng tin tham chi·∫øu t·ªõi payload
        relative_path = path.relative_to(self.frame_storage_dir)
        message = {
            "camera_id": camera_id,
            "timestamp": timestamp,
            "frame_path": str(relative_path),
            "frame_size": file_size,
            "format": "jpeg",
            "message_type": "frame",
        }
        if metadata:
            message["metadata"] = metadata

        # B∆∞·ªõc 3: G·ª≠i "claim check" (message nh·ªè) t·ªõi Kafka
        sent = await self._send_with_retry(message, camera_id)
        if sent:
            async with self.stats_lock: # B·∫£o v·ªá vi·ªác ghi v√†o self.stats
                self.stats[camera_id]["frames"] += 1
        return sent

    async def publish_event(self, camera_id: str, event_type: str, event_data: dict) -> bool:
        """Publish m·ªôt message s·ª± ki·ªán chung l√™n Kafka."""
        timestamp = int(time.time() * 1000)
        message = {
            "camera_id": camera_id,
            "timestamp": timestamp,
            "message_type": event_type,
            "data": event_data
        }
        sent = await self._send_with_retry(message, camera_id)
        if sent:
            async with self.stats_lock: # B·∫£o v·ªá vi·ªác ghi v√†o self.stats
                self.stats[camera_id]["events"] += 1
        return sent

    async def publish_status(self, camera_id: str, status: str, details: dict = None) -> bool:
        """Publish m·ªôt message tr·∫°ng th√°i ƒë·∫∑c bi·ªát, bao g·ªìm c·∫£ th·ªëng k√™ hi·ªán t·∫°i."""
        async with self.stats_lock:
            # T·∫°o b·∫£n sao c·ªßa stats ƒë·ªÉ tr√°nh thay ƒë·ªïi ngo√†i √Ω mu·ªën
            current_stats = self.stats.get(camera_id, {}).copy()

        status_data = {
            "status": status,
            "details": details or {},
            "stats": current_stats
        }
        return await self.publish_event(camera_id, "status", status_data)

    # ------------------- Logic D·ªçn D·∫πp (Cleanup) -------------------

    async def _cleanup_old_files(self):
        """
        D·ªçn d·∫πp c√°c file frame c≈© d·ª±a tr√™n tu·ªïi v√† t·ªïng dung l∆∞·ª£ng.
        H√†m n√†y ƒë∆∞·ª£c t·ªëi ∆∞u ƒë·ªÉ ch·ªâ duy·ªát h·ªá th·ªëng file m·ªôt l·∫ßn.
        """
        logger.info("üßπ B·∫Øt ƒë·∫ßu qu√° tr√¨nh d·ªçn d·∫πp file...")
        try:
            cutoff_time = time.time() - self.max_storage_days * 24 * 3600
            max_bytes = self.max_storage_gb * 1024 * 1024 * 1024

            files_to_check = []
            total_size_bytes = 0
            deleted_by_age = 0
            
            # B∆∞·ªõc 1: Duy·ªát c√¢y th∆∞ m·ª•c M·ªòT L·∫¶N DUY NH·∫§T
            for f in self.frame_storage_dir.rglob("*.jpg"):
                if not f.is_file():
                    continue
                try:
                    stat = f.stat()
                    # X√≥a ngay nh·ªØng file qu√° tu·ªïi
                    if stat.st_mtime < cutoff_time:
                        f.unlink()
                        deleted_by_age += 1
                    else:
                        # Thu th·∫≠p th√¥ng tin c√°c file c√≤n l·∫°i ƒë·ªÉ ki·ªÉm tra dung l∆∞·ª£ng
                        files_to_check.append({'path': f, 'mtime': stat.st_mtime, 'size': stat.st_size})
                        total_size_bytes += stat.st_size
                except FileNotFoundError:
                    continue # File ƒë√£ b·ªã x√≥a b·ªüi m·ªôt ti·∫øn tr√¨nh kh√°c
                except Exception as e:
                    logger.warning(f"Kh√¥ng th·ªÉ x·ª≠ l√Ω file {f}: {e}")

            if deleted_by_age > 0:
                logger.info(f"   - ƒê√£ x√≥a {deleted_by_age} file qu√° {self.max_storage_days} ng√†y tu·ªïi.")

            # B∆∞·ªõc 2: N·∫øu dung l∆∞·ª£ng v·∫´n v∆∞·ª£t m·ª©c, x√≥a c√°c file c≈© nh·∫•t
            deleted_by_size = 0
            if total_size_bytes > max_bytes:
                logger.info(f"   - Dung l∆∞·ª£ng v∆∞·ª£t m·ª©c cho ph√©p ({total_size_bytes / (1024**3):.2f}GB > {self.max_storage_gb:.2f}GB). B·∫Øt ƒë·∫ßu x√≥a file c≈© nh·∫•t.")
                # S·∫Øp x·∫øp c√°c file c√≤n l·∫°i theo th·ªùi gian s·ª≠a ƒë·ªïi (c≈© nh·∫•t tr∆∞·ªõc)
                files_to_check.sort(key=lambda x: x['mtime'])
                
                # M·ª•c ti√™u dung l∆∞·ª£ng l√† 85% m·ª©c t·ªëi ƒëa ƒë·ªÉ c√≥ kho·∫£ng tr·ªëng
                target_size_bytes = max_bytes * 0.85
                
                for file_info in files_to_check:
                    if total_size_bytes <= target_size_bytes:
                        break
                    try:
                        file_info['path'].unlink()
                        total_size_bytes -= file_info['size']
                        deleted_by_size += 1
                    except FileNotFoundError:
                        continue
                    except Exception as e:
                        logger.warning(f"Kh√¥ng th·ªÉ x√≥a file {file_info['path']}: {e}")
            
            total_deleted = deleted_by_age + deleted_by_size
            final_storage_gb = total_size_bytes / (1024**3)
            logger.info(f"‚úÖ D·ªçn d·∫πp ho√†n t·∫•t. T·ªïng c·ªông ƒë√£ x√≥a {total_deleted} file. Dung l∆∞·ª£ng hi·ªán t·∫°i: {final_storage_gb:.2f}GB")

        except Exception as e:
            logger.error(f"L·ªói nghi√™m tr·ªçng trong qu√° tr√¨nh d·ªçn d·∫πp: {e}", exc_info=True)

    async def _periodic_cleanup(self):
        """Ch·∫°y t√°c v·ª• d·ªçn d·∫πp theo ƒë·ªãnh k·ª≥."""
        while self.running:
            try:
                await asyncio.sleep(self.cleanup_interval)
                await self._cleanup_old_files()
            except asyncio.CancelledError:
                logger.info("T√°c v·ª• d·ªçn d·∫πp ƒë·ªãnh k·ª≥ ƒë√£ b·ªã h·ªßy.")
                break
            except Exception as e:
                logger.error(f"L·ªói trong v√≤ng l·∫∑p d·ªçn d·∫πp: {e}", exc_info=True)

    async def close(self):
        """D·ªçn d·∫πp t√†i nguy√™n v√† ƒë√≥ng k·∫øt n·ªëi m·ªôt c√°ch an to√†n."""
        logger.info("B·∫Øt ƒë·∫ßu t·∫Øt Kafka publisher...")
        self.running = False
        if self.cleanup_task:
            self.cleanup_task.cancel()
            try:
                await self.cleanup_task
            except asyncio.CancelledError:
                pass # ƒê√¢y l√† ƒëi·ªÅu mong ƒë·ª£i

        if self.producer:
            await self.producer.flush() # ƒê·∫£m b·∫£o m·ªçi message trong buffer ƒë∆∞·ª£c g·ª≠i ƒëi
            await self.producer.stop()
            logger.info("Kafka producer ƒë√£ d·ª´ng.")
        
        if self.admin_client:
            await self.admin_client.close()
            logger.info("Kafka admin client ƒë√£ ƒë√≥ng.")
        
        if self.executor:
            self.executor.shutdown(wait=True)
            logger.info("Thread pool executor ƒë√£ d·ª´ng.")
